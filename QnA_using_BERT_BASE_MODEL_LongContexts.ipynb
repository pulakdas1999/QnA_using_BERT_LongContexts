{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Importing the BERT Tokenizer**"
      ],
      "metadata": {
        "id": "je7VXDWisJtV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vk0msmhNoUAc"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Import the dataset and reframe it as per the goal of the project**"
      ],
      "metadata": {
        "id": "IkwQc6RcsQC5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_json('data.json')\n",
        "\n",
        "dataset = {'title':[], 'paragraph': [], 'context': [], 'question': [], 'answer': [], 'answer_start': []}\n",
        "for i in range(df['data'].shape[0]):\n",
        "  paragraph_count = 0\n",
        "  for j in range(len(df['data'][i]['paragraphs'])):\n",
        "    for k in range(len(df['data'][i]['paragraphs'][j]['qas'])):\n",
        "      for l in range(len(df['data'][i]['paragraphs'][j]['qas'][k]['answers'])):\n",
        "        dataset['title'].append(df['data'][i]['title'])\n",
        "        dataset['paragraph'].append(paragraph_count)\n",
        "        dataset['context'].append(df['data'][i]['paragraphs'][j]['context'])\n",
        "        dataset['question'].append(df['data'][i]['paragraphs'][j]['qas'][k]['question'])\n",
        "        dataset['answer'].append(df['data'][i]['paragraphs'][j]['qas'][k]['answers'][l]['text'])\n",
        "        dataset['answer_start'].append(df['data'][i]['paragraphs'][j]['qas'][k]['answers'][l]['answer_start'])\n",
        "    paragraph_count += 1\n",
        "\n",
        "data = pd.DataFrame.from_dict(dataset)\n",
        "\n",
        "train = []\n",
        "for i in range(len(dataset['question'])):\n",
        "    train.append({\n",
        "        'context': dataset['context'][i],\n",
        "        'question': dataset['question'][i],\n",
        "        'answer': {\n",
        "            'text': dataset['answer'][i],\n",
        "            'answer_start': dataset['answer_start'][i]}\n",
        "        })\n",
        "\n",
        "train[0]"
      ],
      "metadata": {
        "id": "UWiZFkA5sSwK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Definition to pre-process the training data**"
      ],
      "metadata": {
        "id": "HwGWoTWgsV2D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We need the input as follows ==> [<START> \"Question here...\" <SEP> \"Context here...\" <END>]\n",
        "# And the output should be like ==> [{start-of-answer: <INT>, end-of-answer: <INT>}]\n",
        "\n",
        "def preprocess(list_items):\n",
        "  # Tokenizing the question and context\n",
        "  # Pointers:\n",
        "  # 1. max_length: To restrict the \"question + context\" length to 384\n",
        "  # 2. truncation: To truncate from the \"context\" part off the token if length exceeds 384\n",
        "  # 3. return_offsets_mapping: offset_mapping is ==> String: \"This is a sentence\" --> Token: [<START>, 0, 1, 2, 3, <END>] --> offset_list: [(0, 0), (0, 4), (5, 7), (8, 9), (10, 17), (18, 18)]\n",
        "  inputs = tokenizer(\n",
        "        list_items['question'],\n",
        "        list_items[\"context\"],\n",
        "        max_length=384,\n",
        "        truncation=\"only_second\",\n",
        "        return_offsets_mapping=True,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "\n",
        "  # The code below is to store the start and end positions of the answers\n",
        "  ##############################################################################\n",
        "  sequence_ids = inputs.sequence_ids(0)\n",
        "\n",
        "  offset_mapping = inputs.pop(\"offset_mapping\")\n",
        "  answer = list_items[\"answer\"]\n",
        "  start_positions = 0\n",
        "  end_positions = 0\n",
        "\n",
        "  start_char = answer[\"answer_start\"]\n",
        "  end_char = answer[\"answer_start\"] + len(answer[\"text\"])\n",
        "\n",
        "  # Find the start and end of the context\n",
        "  idx = 0\n",
        "  while sequence_ids[idx] != 1:\n",
        "    idx += 1\n",
        "  context_start = idx\n",
        "  while sequence_ids[idx] == 1:\n",
        "    idx += 1\n",
        "  context_end = idx - 1\n",
        "\n",
        "  # If the answer is not fully inside the context, label it (0, 0)\n",
        "  if offset_mapping[context_start][0] > end_char or offset_mapping[context_end][1] < start_char:\n",
        "    start_positions = 0\n",
        "    end_positions = 0\n",
        "  else:\n",
        "    # Otherwise it's the start and end token positions\n",
        "    idx = context_start\n",
        "    while idx <= context_end and offset_mapping[idx][0] <= start_char:\n",
        "        idx += 1\n",
        "    start_positions = (idx - 1)\n",
        "\n",
        "    idx = context_end\n",
        "    while idx >= context_start and offset_mapping[idx][1] >= end_char:\n",
        "      idx -= 1\n",
        "    end_positions = (idx + 1)\n",
        "\n",
        "  inputs[\"start_positions\"] = start_positions\n",
        "  inputs[\"end_positions\"] = end_positions\n",
        "  ##############################################################################\n",
        "\n",
        "  return inputs"
      ],
      "metadata": {
        "id": "WCpm_ZN9sZEw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Using 'map' to apply the definition made above to every training data**"
      ],
      "metadata": {
        "id": "bLSN5277scss"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_items = list(map(preprocess, train))"
      ],
      "metadata": {
        "id": "rjqVzom0sgMD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Importing the 'model' and Preparing the training data**"
      ],
      "metadata": {
        "id": "1Jykm-UssiqF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TFAutoModelForQuestionAnswering\n",
        "model = TFAutoModelForQuestionAnswering.from_pretrained(\"distilbert/distilbert-base-uncased\")"
      ],
      "metadata": {
        "id": "nCn-WdGUslYy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Preparing a dictionary of the training data**"
      ],
      "metadata": {
        "id": "RHFNk6wosnus"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "X = {'input_ids': [], 'attention_mask': []}\n",
        "Y = {'start_positions': [], 'end_positions': []}\n",
        "\n",
        "for i in tokenized_items:\n",
        "  X['input_ids'].append(np.array(i['input_ids']))\n",
        "  X['attention_mask'].append(np.array(i['attention_mask']))\n",
        "\n",
        "  Y['start_positions'].append(i['start_positions'])\n",
        "  Y['end_positions'].append(i['end_positions'])\n",
        "\n",
        "X['input_ids'] = np.array(X['input_ids'])\n",
        "X['attention_mask'] = np.array(X['attention_mask'])\n",
        "\n",
        "Y['start_positions'] = np.array(Y['start_positions'])\n",
        "Y['end_positions'] = np.array(Y['end_positions'])"
      ],
      "metadata": {
        "id": "MPs06ZKvsqwN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Creating the optimizer before training**"
      ],
      "metadata": {
        "id": "luGdDdBEstLH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import create_optimizer\n",
        "\n",
        "batch_size = 16\n",
        "num_epochs = 1\n",
        "total_train_steps = (len(tokenized_items) // batch_size) * num_epochs\n",
        "optimizer, schedule = create_optimizer(\n",
        "    init_lr=2e-5,\n",
        "    num_warmup_steps=0,\n",
        "    num_train_steps=total_train_steps,\n",
        ")"
      ],
      "metadata": {
        "id": "IyaxeODzswa9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Compiling the model created**"
      ],
      "metadata": {
        "id": "M2zNxvvIsyyB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "model.compile(optimizer=optimizer)\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "qICA5uHes2Dq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Training**"
      ],
      "metadata": {
        "id": "cipP5Tdas4B_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(x=X, y=Y, epochs=1)"
      ],
      "metadata": {
        "id": "lJUDjqYIs60l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Validating the output**"
      ],
      "metadata": {
        "id": "T63znbnPs84w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "context = \"\"\"\n",
        "The hall housed multiple classrooms and science labs needed for early research at the university. In 1919 Father James Burns became president of\n",
        "Notre Dame, and in three years he produced an academic revolution that brought the school up to national standards by adopting the elective system\n",
        "and moving away from the university's traditional scholastic and classical emphasis. By contrast, the Jesuit colleges, bastions of academic\n",
        "conservatism, were reluctant to move to a system of electives. Their graduates were shut out of Harvard Law School for that reason. Notre Dame\n",
        "continued to grow over the years, adding more colleges, programs, and sports teams. By 1921, with the addition of the College of Commerce, Notre Dame\n",
        "had grown from a small college to a university with five colleges and a professional law school. The university continued to expand and add new\n",
        "residence halls and buildings with each subsequent president. One of the main driving forces in the growth of the University was its football team,\n",
        "the Notre Dame Fighting Irish. Knute Rockne became head coach in 1918. Under Rockne, the Irish would post a record of 105 wins, 12 losses, and five\n",
        "ties. During his 13 years the Irish won three national championships, had five undefeated seasons, won the Rose Bowl in 1925, and produced players\n",
        "such as George Gipp and the \"Four Horsemen\". Knute Rockne has the highest winning percentage (.881) in NCAA Division I/FBS football history. Rockne's\n",
        "offenses employed the Notre Dame Box and his defenses ran a 7–2–2 scheme. The last game Rockne coached was on December 14, 1930 when he led a group of\n",
        "Notre Dame all-stars against the New York Giants in New York City.\n",
        "\"\"\"\n",
        "questions = [\"What was the amount of wins Knute Rockne attained at Notre Dame while head coach?\",\n",
        "            \"Over how many years did the change to national standards undertaken at Notre Dame in the early 20th century take place?\"]\n",
        "\n",
        "for question in questions:\n",
        "  if len(context) < 100:\n",
        "    inputs = tokenizer(question, context, return_tensors=\"tf\")\n",
        "    outputs = model(**inputs)\n",
        "    answer_start_index = int(tf.math.argmax(outputs.start_logits, axis=-1)[0])\n",
        "    answer_end_index = int(tf.math.argmax(outputs.end_logits, axis=-1)[0])\n",
        "    predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]\n",
        "    print(tokenizer.decode(predict_answer_tokens))\n",
        "  else:\n",
        "    ans = \"Nothing to print!!\"\n",
        "    Weight = float('-inf')\n",
        "    for i in range(100, len(context.split())):\n",
        "      temp = \" \".join(context.split()[i-100 : i+1])\n",
        "      inputs = tokenizer(question, temp, return_tensors=\"tf\")\n",
        "      outputs = model(**inputs)\n",
        "      if Weight < (int(max(outputs.start_logits[0])) + int(max(outputs.end_logits[0]))):\n",
        "        Weight = (int(max(outputs.start_logits[0])) + int(max(outputs.end_logits[0])))\n",
        "        answer_start_index = int(tf.math.argmax(outputs.start_logits, axis=-1)[0])\n",
        "        answer_end_index = int(tf.math.argmax(outputs.end_logits, axis=-1)[0])\n",
        "        predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]\n",
        "        ans = tokenizer.decode(predict_answer_tokens)\n",
        "      print(ans)"
      ],
      "metadata": {
        "id": "Lg8DwhgbBX6i"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}